---
title: "Classification of quality of barbell lifts based on wearable sensor data"
author: "Vivek Yadav"
date: "Saturday, September 20, 2014"
output:
  html_document:
    keep_md: yes
  pdf_document: default
---
### Synopsis

This report presents implentation of a machine learning based algorithm to classify barbell lifting data obtained from wearable sensor. First raw data was processed to discard non-kineamtic data and remove variables without varitions. This resulted in a data set that was different for different people. Further, the raw kinematic (position-orientation-accelration-velocity etc) data depends on how the sensor is worn. To avoid variations due to differences in how sensors were worn, and differences in individual devices, a principal component (PCA) followed by support vector machine (SVM) model was fit to data of each person. PCA+SVM method was chosen because other methods like tree or rpart did not provide good classification. Collected data was split into training (40%), test (40%) and validation sets (20%). Cross-validation was perfomed using random sampling without replacements. To avoid issues of overfittig, a consensus based-model was implemented where 41 different models were fit to data, and consensus from these 41 models was used to identify the best solution. Overall, a good fit to data was observed (>98% accuracy) for all subjects, and all 20 final test cases were predicted correctly by the algorithm. 

#### Data was processed in following steps, 


```{r echo=FALSE,message =FALSE,cache=TRUE}

suppressWarnings(library(caret))
suppressWarnings(library(tree))
suppressWarnings(library(rpart))
suppressWarnings(library(e1071))


data_train<-read.csv('pml-training.csv')
data_test<-read.csv('pml-testing.csv')



ind_na = is.na(data_train[1,])
data_train2 <- as.data.frame(data_train[,ind_na==FALSE])
data_test2 <- as.data.frame(data_test[,ind_na==FALSE])


```


1. First step was to exclude data that was non-numeric and was not collected. All columns without data were removed. Futher,there were spelling mistakes in 3 rows where 'pitch' was written as 'picth'.
2. Some people had some data missing. For example, jeremy had roll, pitch and yaw for arm (col: 15-17) missing, whereas adelmo had these measures for forearm (col:41-43) missing. 
3. Separate PCA-SVM model was fit to data of each person. PCAs were computed so as to explain 99% of covariance, this resulted in about 30 (against 52) predictors. 
4. Collected data was split into mode;-building (80%) and validation set (20%). 
5. In cross-validation, model-building data was randomly divided into equal parts, and 41 different models were fit.
6. Consensus between these models was chosen to obtain prediction. 
7. Accuracy and performance of model was tested and reported on validation set.

```{r, echo=FALSE,message =FALSE,cache=TRUE}

features<-names(data_train2)
roll_ind<-grep("_roll_",features,fixed = TRUE) 
pitch_ind<-grep("_pitch_",features,fixed = TRUE)
picth_ind<-grep("_picth_",features,fixed = TRUE)
yaw_ind<-grep("_yaw_",features,fixed = TRUE)

# roll_arm, pitch_arm, yaw_arm
#roll_ind<-grep("roll_arm",features,fixed = TRUE)
#pitch_ind<-grep("pitch_arm",features,fixed = TRUE)
#yaw_aind<-grep("yaw_arm",features,fixed = TRUE)

data_train2 <- as.data.frame( data_train2[, -c( roll_ind, pitch_ind, yaw_ind, picth_ind ) ] )
data_train3 <- data_train2[,-c(1,3,4,5,6,7)]

data_usr<- split(data_train3,data_train3$user_name)
AllData<- data_usr$pedro

ind_all_predictors<- 2:53
ind_bad_predictors<- ind_all_predictors[apply(AllData[,2:53],2,sd)==0]
if (length(ind_bad_predictors)!=0) {

  ind_predictors <-ind_all_predictors[-ind_bad_predictors]
  AllData<- AllData[,-ind_bad_predictors]
}
predictors<-names(AllData)
predictors<-predictors[2:(dim(AllData)[2]-1)]

ind_all <- sample(1:dim(AllData)[1])
num_model <- round(length(ind_all)*.8)
indModel <- ind_all[1:(num_model)]

data_training <- AllData[indModel,]
data_CV       <- AllData[-indModel,]


```

### Results

#### Principal components

``` {r echo =FALSE, message=FALSE, cache=TRUE,fig.width=5,fig.height=4}
library(stats)
pc_all<-princomp(data_training[predictors])
preProc<-preProcess(data_training[predictors],method = "pca",thresh=0.99)
predictors_PC<- predict(preProc,data_training[predictors])
AllData_PC <- cbind(predictors_PC,classe = data_training[,dim(data_training)[2]])

sd_pc= vector()
for (i in 1:30) {sd_pc[i] = sum(pc_all$sdev[1:i])/sum(pc_all$sdev)} 
plot(sd_pc,type="l",
     xlab="number of PCs",
     ylab="Covariance explained by PCs") 
 
```

Figure 1. Covarince explained by each variable. Cut off used here was 99% which gave about 30 PCs. 



``` {r echo =FALSE, message=FALSE, cache=TRUE,fig.height=4}
  par(mfrow=c(1,2))
  plot(predictors_PC$PC1,predictors_PC$PC2,col=data_training$classe,
       xlab ="PC1",ylab="PC2")
  plot(predictors_PC$PC3,predictors_PC$PC4,col=data_training$classe, 
       xlab ="PC3",ylab="PC4")
```

Figure 2. The plots above indicate that it is may be possible to use classify classes (color coded) based on PCs.

41 svm models were fit to data. For each model, model-building set was divided into 2 equal parts, and svm was used to classify. A consensus between these models was used to obtain prediction from our model. 

### SVM performance on validation set. 

41 svm models were fit to data, a consensus between these models was used to obtain final prediction. The "odd" number 41 was chosen to avoid ties, although highly unlikely. 



``` {r echo=FALSE, message=FALSE, cache=TRUE}
predictors_PC_cv<- predict(preProc,data_CV[predictors])
AllData_PC_cv   <- cbind(predictors_PC_cv,classe = data_CV[,dim(data_training)[2]])

inTrain <- createDataPartition(y=AllData_PC$classe, p = .5,list = FALSE)
num_train = length(inTrain)
trainData<- AllData_PC[inTrain,] 
testData <- AllData_PC[-inTrain,]

len_data<- dim(AllData_PC)[2]


## algorithm selection
svm_model <- svm(classe ~ ., data = trainData)
pred_svm <- predict(svm_model, testData[,-len_data])
#confusionMatrix(pred_svm,testData[,"classe"])

rpart_model <- rpart(classe ~ ., data = trainData)
pred_rpart <- predict(rpart_model, testData[,-len_data],type = "class")
#confusionMatrix(pred_rpart,testData[,"classe"])

tree_model <- tree(classe ~ ., data = trainData)
pred_tree <- predict(tree_model, testData[,-len_data],type = "class") 
#confusionMatrix(pred_tree,testData[,"classe"])

```

``` {r echo=FALSE, message=FALSE, cache=TRUE}
SvmFit_all <- list() 
ConfMat_all <- list() 

num_fits = 41 
for (j in 1:num_fits) {
  indModel <- sample(indModel,replace = TRUE)
  indTrain <- ind_all[1:(num_train)]
  indTest <- ind_all[1:(num_train)]
  
  inTrain <- createDataPartition(y=AllData_PC$classe, p = .7,list = FALSE)
  trainData<- AllData_PC[inTrain,]
  testData <- AllData_PC[-inTrain,]
  
  svm_model <- svm(classe ~ ., data = trainData)
  pred_svm <- predict(svm_model, testData[,-dim(testData)[2]]) 
  ConfMat_all[[j]]=confusionMatrix(pred_svm,testData[,"classe"])
  SvmFit_all[[j]] <-svm_model
}

mat_ind <- c("A","B","C","D","E")


mat_pred <- as.data.frame(matrix(nrow = dim(AllData_PC_cv)[1], ncol = num_fits))
for (j in 1:num_fits){
  pred_svm <- predict(SvmFit_all[[j]], AllData_PC_cv[,-len_data])
  mat_pred[,j] <- pred_svm
}

mat_cons <- as.data.frame(matrix(nrow = dim(AllData_PC_cv)[1], ncol = 5))
for (j in 1:5) {
  a<- mat_pred == mat_ind[j]
  mat_cons[,j] <- apply(a, 1, sum)  
}
mn_exp <- (apply(mat_cons, 1,which.max))
mat_cons_all<-apply(mat_cons,1,max)/41 

hist(mat_cons_all[mat_cons_all!=1],col="blue",
     xlab = "% of models agreeing on one value",
     ylab = "Frequenchy",
     main = "")
 
num_cons<-length(mat_cons_all)  
num_conf<-length(mat_cons_all[mat_cons_all!=1])

```

Figure 3. Histogram of % of models that agreed on one prediction. 

In `r num_cons` number of cases in the validation set, there was disagreement between models in `r num_conf` cases. This was resolved using consensus between the models. Overall, the confusion matrix is reported below. This resulted in expected accuracy > 98%. 


```{r cache=TRUE,echo=FALSE}

conf_mat<- confusionMatrix(mat_ind[mn_exp],AllData_PC_cv[,len_data])
print(conf_mat)

```


